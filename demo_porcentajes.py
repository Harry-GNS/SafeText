"""
DemostraciÃ³n de cÃ³mo se calculan los porcentajes de seguridad en SafeText.
Muestra el proceso completo desde patrones hasta porcentajes finales.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from algorithms.analyzer import CyberbullyingAnalyzer

def demo_safety_calculation():
    """
    Demuestra cÃ³mo se calculan los porcentajes de seguridad.
    """
    analyzer = CyberbullyingAnalyzer()
    
    # Casos de prueba con diferentes niveles de severidad
    casos_prueba = [
        {
            'nombre': 'Contenido Completamente Seguro',
            'texto': 'Hola, Â¿cÃ³mo estÃ¡s? Espero que tengas un buen dÃ­a. Nos vemos maÃ±ana en clase.'
        },
        {
            'nombre': 'Riesgo Bajo - Insulto Leve',
            'texto': 'No seas tonto, eso no es asÃ­. Pero bueno, todos nos equivocamos.'
        },
        {
            'nombre': 'Riesgo Medio - Varios Insultos',
            'texto': 'Eres un idiota y estÃºpido. No entiendes nada. Mejor cÃ¡llate.'
        },
        {
            'nombre': 'Riesgo Alto - Insultos Graves',
            'texto': 'Eres un inÃºtil, no sirves para nada. Nadie te quiere aquÃ­. Eres patÃ©tico.'
        },
        {
            'nombre': 'Riesgo CrÃ­tico - Amenazas',
            'texto': 'Eres un inÃºtil, no sirves para nada. Vete a morir. Te voy a pegar si no te callas.'
        }
    ]
    
    print("=" * 100)
    print("DEMOSTRACIÃ“N: CÃLCULO DE PORCENTAJES DE SEGURIDAD")
    print("=" * 100)
    
    for i, caso in enumerate(casos_prueba, 1):
        print(f"\n{i}. {caso['nombre'].upper()}")
        print("-" * 80)
        print(f"Texto: '{caso['texto']}'")
        
        # Realizar anÃ¡lisis
        resultado = analyzer.analyze_text(caso['texto'])
        
        print(f"\nðŸ“Š RESULTADOS DEL ANÃLISIS:")
        print(f"   â€¢ Es ciberacoso: {resultado['is_cyberbullying']}")
        print(f"   â€¢ Nivel de riesgo: {resultado['risk_level']}")
        print(f"   â€¢ Patrones detectados: {resultado['total_patterns_found']}")
        print(f"   â€¢ Total coincidencias: {resultado['total_matches']}")
        
        # Mostrar distribuciÃ³n de severidad
        if resultado['severity_summary']:
            print(f"\nðŸŽ¯ DISTRIBUCIÃ“N DE SEVERIDAD:")
            for severity, count in resultado['severity_summary'].items():
                if count > 0:
                    print(f"   â€¢ {severity}: {count} coincidencia(s)")
        
        # Calcular y mostrar porcentajes
        porcentajes = calcular_porcentajes_seguridad(resultado)
        
        print(f"\nðŸ“ˆ PORCENTAJES DE SEGURIDAD:")
        print(f"   ðŸŸ¢ Contenido Seguro: {porcentajes['seguro']}%")
        print(f"   ðŸŸ¡ Contenido Sospechoso: {porcentajes['sospechoso']}%")
        print(f"   ðŸ”´ Contenido Peligroso: {porcentajes['peligroso']}%")
        
        # Explicar el cÃ¡lculo
        print(f"\nðŸ§® EXPLICACIÃ“N DEL CÃLCULO:")
        if not resultado['is_cyberbullying']:
            print("   â†’ Sin ciberacoso detectado = 100% seguro")
        else:
            print(f"   â†’ Nivel de riesgo '{resultado['risk_level']}' determina la distribuciÃ³n:")
            explicar_calculo_porcentajes(resultado['risk_level'])
        
        print("\n" + "=" * 100)

def calcular_porcentajes_seguridad(resultado):
    """
    Replica la lÃ³gica del frontend para calcular porcentajes.
    """
    if not resultado['is_cyberbullying']:
        return {'seguro': 100, 'sospechoso': 0, 'peligroso': 0}
    
    # LÃ³gica basada en el nivel de riesgo (del archivo resultados.html)
    nivel_riesgo = resultado['risk_level']
    
    if nivel_riesgo == 'Low':
        return {'seguro': 70, 'sospechoso': 30, 'peligroso': 0}
    elif nivel_riesgo == 'Medium':
        return {'seguro': 50, 'sospechoso': 40, 'peligroso': 10}
    elif nivel_riesgo == 'High':
        return {'seguro': 30, 'sospechoso': 30, 'peligroso': 40}
    elif nivel_riesgo == 'Critical':
        return {'seguro': 10, 'sospechoso': 20, 'peligroso': 70}
    else:
        return {'seguro': 90, 'sospechoso': 10, 'peligroso': 0}

def explicar_calculo_porcentajes(nivel_riesgo):
    """
    Explica cÃ³mo se determinan los porcentajes para cada nivel.
    """
    explicaciones = {
        'Low': """
        â€¢ Riesgo Bajo: Insultos leves detectados
        â€¢ 70% Seguro: La mayor parte del contenido es apropiado
        â€¢ 30% Sospechoso: Algunos elementos requieren atenciÃ³n
        â€¢ 0% Peligroso: Sin contenido altamente daÃ±ino
        """,
        'Medium': """
        â€¢ Riesgo Medio: MÃºltiples insultos o contenido moderadamente agresivo
        â€¢ 50% Seguro: Mitad del contenido es problemÃ¡tico
        â€¢ 40% Sospechoso: Contenido que requiere intervenciÃ³n
        â€¢ 10% Peligroso: Algunos elementos daÃ±inos presentes
        """,
        'High': """
        â€¢ Riesgo Alto: Insultos graves, exclusiÃ³n social, ataques personales
        â€¢ 30% Seguro: MayorÃ­a del contenido es problemÃ¡tico
        â€¢ 30% Sospechoso: Contenido ambiguo que necesita revisiÃ³n
        â€¢ 40% Peligroso: Gran cantidad de contenido daÃ±ino
        """,
        'Critical': """
        â€¢ Riesgo CrÃ­tico: Amenazas, deseos de muerte, violencia
        â€¢ 10% Seguro: Muy poco contenido apropiado
        â€¢ 20% Sospechoso: Contenido menor comparado con lo grave
        â€¢ 70% Peligroso: MayorÃ­a del contenido es extremadamente daÃ±ino
        """
    }
    
    print(explicaciones.get(nivel_riesgo, "Nivel no reconocido"))

def demo_scoring_algorithm():
    """
    Demuestra cÃ³mo funciona el algoritmo de puntuaciÃ³n ponderada.
    """
    print("\n" + "=" * 100)
    print("ALGORITMO DE PUNTUACIÃ“N PONDERADA")
    print("=" * 100)
    
    # Pesos del sistema (del analyzer.py)
    pesos = {'Low': 1, 'Medium': 3, 'High': 7, 'Critical': 15}
    
    print("ðŸ”¢ SISTEMA DE PESOS:")
    for severity, peso in pesos.items():
        print(f"   â€¢ {severity}: {peso} puntos")
    
    print("\nðŸ“‹ UMBRALES DE CLASIFICACIÃ“N:")
    print("   â€¢ 0-2 puntos â†’ Riesgo LOW")
    print("   â€¢ 3-6 puntos â†’ Riesgo MEDIUM") 
    print("   â€¢ 7-14 puntos â†’ Riesgo HIGH")
    print("   â€¢ 15+ puntos â†’ Riesgo CRITICAL")
    
    # Ejemplos de cÃ¡lculo
    ejemplos = [
        {'Low': 2, 'Medium': 0, 'High': 0, 'Critical': 0},  # 2 puntos = Low
        {'Low': 0, 'Medium': 2, 'High': 0, 'Critical': 0},  # 6 puntos = Medium
        {'Low': 1, 'Medium': 1, 'High': 1, 'Critical': 0},  # 11 puntos = High
        {'Low': 0, 'Medium': 0, 'High': 0, 'Critical': 1},  # 15 puntos = Critical
    ]
    
    print("\nðŸ§® EJEMPLOS DE CÃLCULO:")
    for i, ejemplo in enumerate(ejemplos, 1):
        total = sum(ejemplo[severity] * pesos[severity] for severity in pesos)
        
        print(f"\nEjemplo {i}:")
        print("   Detecciones:", end=" ")
        for severity, count in ejemplo.items():
            if count > 0:
                print(f"{count}x{severity}({pesos[severity]}pts)", end=" ")
        
        print(f"\n   CÃ¡lculo: {' + '.join(f'{count}Ã—{pesos[severity]}' for severity, count in ejemplo.items() if count > 0)} = {total} puntos")
        
        # Determinar clasificaciÃ³n
        if total >= 15:
            clasificacion = 'CRITICAL'
        elif total >= 7:
            clasificacion = 'HIGH'
        elif total >= 3:
            clasificacion = 'MEDIUM'
        else:
            clasificacion = 'LOW'
        
        print(f"   Resultado: {total} puntos â†’ Riesgo {clasificacion}")

if __name__ == '__main__':
    demo_safety_calculation()
    demo_scoring_algorithm()
